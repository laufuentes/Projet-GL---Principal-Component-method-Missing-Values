{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.experimental import enable_iterative_imputer  \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from src.utils import *\n",
    "from src.algorithms import * \n",
    "from src.metrics_FAMD import *\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction to compute metrics for the generated datasets\n",
    "def compute_metrics(df, cat_idx, n_it, n_components, proba_non_missing):\n",
    "    \"\"\" Compute metrics for different missing probabilities     \n",
    "    Args: \n",
    "        df (pd.DataFrame): Dataframe to  \n",
    "        proba_non_missing (list of float) : List of probabilities that a value is not missing\n",
    "\n",
    "    Returns:\n",
    "        data_missing_raw (pd.DataFrame): Masked dataframe hence containing missing values \n",
    "\n",
    "    \"\"\"\n",
    "    # Categorical Variables :\n",
    "    idx_k2 = pd.Index(cat_idx)\n",
    "    # Continuous Variables\n",
    "    idx_k1 = df.columns.difference(idx_k2)\n",
    "\n",
    "    dict_dfs = {}\n",
    "    for p in proba_non_missing: \n",
    "        df_missing = create_missingness(df, p)\n",
    "        \n",
    "        # Encode dummy variables in the dataframe and in the dataframe with missing values :\n",
    "        df_missing_dummy, idx_j, nb_values_per_cat_df = encode_dummy_variables(df_missing, idx_k2)\n",
    "        df_dummy = encode_dummy_variables(df, idx_k2)[0]\n",
    "        dict_dfs.update({p:[idx_k1, idx_j, df_missing_dummy, df_dummy, nb_values_per_cat_df]})\n",
    "        #print(\"proba non missing: \" f'{p}', \"missingness rate: \",df_missing.isna().sum().sum()/(df_missing.shape[0]*df_missing.shape[1]))\n",
    "\n",
    "    #IFAMD\n",
    "    fc_rate = []\n",
    "    nmrse = []\n",
    "\n",
    "    for p,values in dict_dfs.items(): \n",
    "        k1, k_j, df_missing, df_true, nb_val_per_car = values\n",
    "        C0_missing, Categ_missing = df_missing.isna()[k1].to_numpy(), df_missing.isna()[k_j].to_numpy()  \n",
    "        \n",
    "        #Computation of iterative FAMD\n",
    "        ifamd_df = IterativeFAMDImputer(n_components=n_components, data=df_missing, k1=k1, k2=k_j, nb_values_per_cat = nb_val_per_car)\n",
    "        ifamd_df.impute(n_it)\n",
    "        df = ifamd_df.df\n",
    "\n",
    "\n",
    "        # We encode categories into 0,1\n",
    "        res = ifamd_df.df[ifamd_df.k2].copy()\n",
    "        pos = 0\n",
    "        for h in range (len(idx_k2)) :\n",
    "            col = [idx_j[pos+i] for i in range (nb_values_per_cat_df[h])]\n",
    "            res[\"max_value\"] = ifamd_df.df[col].max(axis = 1)\n",
    "            for value in col:\n",
    "                res[value] = (res[value] == res[\"max_value\"]).astype(int)\n",
    "            pos += nb_values_per_cat_df[h]\n",
    "        res = res[ifamd_df.k2] \n",
    "\n",
    "        #Compute metrics \n",
    "        fc_rate.append(metric_fc(res[Categ_missing], df_true[k_j][Categ_missing]))\n",
    "\n",
    "        # For continuous variables: \n",
    "        nmrse.append(compute_nrmse_weighted(df[k1][C0_missing], df_true[k1][C0_missing]))\n",
    "\n",
    "    fc_rate = np.array(fc_rate)\n",
    "    nmrse = np.array(nmrse)\n",
    "\n",
    "    return fc_rate, nmrse, idx_j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed to create reproductible results\n",
    "np.random.seed(21032024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters of the first dataset created in the paper : (3.1 Relationships between continuous and categorical variables)\n",
    "\n",
    "n = 100 # Sample size\n",
    "S = 2  # \n",
    "K = [1,3]  #K[s] = number of times the variable s (s in {1,...,S}) is duplicated in the dataset\n",
    "cat = 2 #number of categorical variables\n",
    "cat_idx = [1,2] #index of the categorical variables\n",
    "nb_of_cat_per_var = [4,4] #number of categories for each categorical variable\n",
    "\n",
    "df_3_1_snr1 = create_dataset(n, S, K, cat, cat_idx, nb_of_cat_per_var, 1)\n",
    "df_3_1_snr3 = create_dataset(n, S, K, cat, cat_idx, nb_of_cat_per_var, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define number of iterations \n",
    "n_it = 1000\n",
    "n_components = 3\n",
    "\n",
    "# Inject missing values into the dataframe :\n",
    "proba_non_missing = [0.7, 0.8, 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_simulations = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n",
      "Converged in 2\n"
     ]
    }
   ],
   "source": [
    "S = 2   \n",
    "K = [1,3]  #K[s] = number of times the variable s (s in {1,...,S}) is duplicated in the dataset\n",
    "cat = 2 #number of categorical variables\n",
    "cat_idx = [1,2] #index of the categorical variables\n",
    "nb_of_cat_per_var = [4,4] #number of categories for each categorical variable\n",
    "\n",
    "mean_fc_rate_list_3_1_snr3 = []\n",
    "rnmse_list_3_1_snr3 = []\n",
    "\n",
    "for n_sim in range(n_simulations):\n",
    "    df_3_1_snr3 = create_dataset(n, S, K, cat, cat_idx, nb_of_cat_per_var, 3)\n",
    "    fc_rate, nmrse, idx_j = compute_metrics(df_3_1_snr3, pd.Index(cat_idx).map(str), n_it, n_components, proba_non_missing)\n",
    "    mean_fc_rate = [np.mean(fc_rate[i]) for i in range(len(proba_non_missing))]\n",
    "    rnmse_list_3_1_snr3.append(nmrse)\n",
    "    mean_fc_rate_list_3_1_snr3.append(mean_fc_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.25, 0.2692307692307692, 0.26136363636363635],\n",
       " [0.33064516129032256, 0.3026315789473684, 0.3076923076923077],\n",
       " [0.2675438596491228, 0.2717391304347826, 0.26041666666666663],\n",
       " [0.26315789473684215, 0.22857142857142856, 0.2647058823529411],\n",
       " [0.2943548387096774, 0.24358974358974358, 0.30625],\n",
       " [0.28515625, 0.2426470588235294, 0.3035714285714286],\n",
       " [0.2579365079365079, 0.28125, 0.25],\n",
       " [0.2916666666666667, 0.2777777777777778, 0.26136363636363635],\n",
       " [0.25943396226415094, 0.30000000000000004, 0.25],\n",
       " [0.31, 0.29583333333333334, 0.2884615384615385],\n",
       " [0.2653846153846154, 0.28888888888888886, 0.3577586206896552],\n",
       " [0.2619047619047619, 0.34444444444444444, 0.2258064516129032],\n",
       " [0.3035714285714286, 0.2441860465116279, 0.3],\n",
       " [0.2647058823529412, 0.2894736842105263, 0.2894736842105263],\n",
       " [0.25735294117647056, 0.24166666666666667, 0.30000000000000004],\n",
       " [0.28409090909090906, 0.29069767441860467, 0.24],\n",
       " [0.2958333333333333, 0.26111111111111107, 0.28125],\n",
       " [0.3194444444444445, 0.27976190476190477, 0.2361111111111111],\n",
       " [0.2992957746478873, 0.26744186046511625, 0.26785714285714285],\n",
       " [0.24107142857142858, 0.23717948717948717, 0.2894736842105263]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_fc_rate_list_3_1_snr3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([9.26987327e+07, 1.19036329e+08, 3.34351001e+07]),\n",
       " array([1.64932710e+08, 6.35629320e+07, 1.19316007e+08]),\n",
       " array([1.10747464e+08, 3.89870850e+07, 5.37069432e+07]),\n",
       " array([38027909.76484077, 41985056.98693085, 90248595.30150166]),\n",
       " array([2.56605762e+07, 1.60431047e+08, 1.49232366e+08]),\n",
       " array([69149674.78995372, 80092921.8872097 , 43579089.66499741]),\n",
       " array([23215381.2935992 , 98311288.50957365, 40875325.3728473 ]),\n",
       " array([2.53680348e+07, 1.25695542e+08, 8.45542258e+07]),\n",
       " array([1.62362346e+08, 2.83996358e+07, 6.63640285e+07]),\n",
       " array([33596071.72729644, 78915601.42319325, 31208741.86409141]),\n",
       " array([6.30792613e+07, 1.68751002e+08, 1.30210740e+08]),\n",
       " array([1.29396549e+08, 1.25705819e+08, 4.77030291e+07]),\n",
       " array([6.84224936e+07, 8.46466306e+07, 1.38919056e+08]),\n",
       " array([1.37825500e+08, 8.07472888e+07, 3.92232367e+08]),\n",
       " array([5.09820817e+07, 7.81146912e+07, 3.25122028e+08]),\n",
       " array([5.25110570e+07, 6.96237682e+07, 1.69197501e+08]),\n",
       " array([62038830.45151004, 47709743.90706981, 74306109.32693347]),\n",
       " array([4.48918352e+07, 1.71551207e+08, 1.20187428e+08]),\n",
       " array([8.26713053e+07, 1.03553137e+08, 7.59801475e+07]),\n",
       " array([1.42934927e+08, 5.39732944e+07, 1.71095479e+07])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnmse_list_3_1_snr3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,5),ncols=4, sharey=True)\n",
    "fig.suptitle('Imputation techniques accuracy for categories')\n",
    "\n",
    "for i, vect in enumerate(mean_fc_rate_list_3_1_snr3): \n",
    "    ax[0].scatter(idx_j_3_1_snr1.to_numpy(), vect, label=proba_non_missing[i])\n",
    "    ax[0].plot(idx_j_3_1_snr1.to_numpy(),np.ones_like(idx_j_3_1_snr1.to_numpy())*vect.mean())\n",
    "    ax[0].set_ylabel(\"Falsely classified rate\")\n",
    "    ax[0].set_title(\"iterative FAMD on data with SNR 1\")\n",
    "    ax[0].legend()\n",
    "\n",
    "for i, vect in enumerate(rnmse_list_3_1_snr3): \n",
    "    ax[1].scatter(idx_j_3_1_snr3.to_numpy(), vect, label=proba_non_missing[i])\n",
    "    ax[1].plot(idx_j_3_1_snr3.to_numpy(),np.ones_like(idx_j_3_1_snr3.to_numpy())*vect.mean())\n",
    "    ax[1].set_ylabel(\"Falsely classified rate\")\n",
    "    ax[1].set_title(\"iterative FAMD on data with SNR 1\")\n",
    "    ax[1].legend()\n",
    "\n",
    "    \n",
    "\n",
    "#fig.savefig('images/PFC_categories.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters of the second dataset created in the paper : (3.2.1 Linear and nonlinear relationships)\n",
    "\n",
    "S = 1   \n",
    "K = [5]  #K[s] = number of times the variable s (s in {1,...,S}) is duplicated in the dataset\n",
    "\n",
    "cat = 3 #number of categorical variables\n",
    "cat_idx = [3,4,5] #index of the categorical variables\n",
    "nb_of_cat_per_var = [4,4] #number of categories for each categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_3_1_snr3\n",
    "\n",
    "cat_idx = pd.Index(cat_idx).map(str)\n",
    "n_it = n_it\n",
    "n_components =2\n",
    "proba_non_missing = [0.7,0.8,0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical Variables :\n",
    "idx_k2 = pd.Index(cat_idx)\n",
    "# Continuous Variables\n",
    "idx_k1 = df.columns.difference(idx_k2)\n",
    "\n",
    "dict_dfs = {}\n",
    "for p in proba_non_missing: \n",
    "    df_missing = create_missingness(df, p)\n",
    "        \n",
    "    # Encode dummy variables in the dataframe and in the dataframe with missing values :\n",
    "    df_missing_dummy, idx_j, nb_values_per_cat_df = encode_dummy_variables(df_missing, idx_k2)\n",
    "    df_dummy = encode_dummy_variables(df, idx_k2)[0]\n",
    "    dict_dfs.update({p:[idx_k1, idx_j, df_missing_dummy, df_dummy, nb_values_per_cat_df]})\n",
    "    #print(\"proba non missing: \" f'{p}', \"missingness rate: \",df_missing.isna().sum().sum()/(df_missing.shape[0]*df_missing.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #IFAMD\n",
    "fc_rate = []\n",
    "nmrse = []\n",
    "\n",
    "for p,values in dict_dfs.items(): \n",
    "    k1, k_j, df_missing, df_true, nb_val_per_car = values\n",
    "    C0_missing, Categ_missing = df_missing.isna()[k1].to_numpy(), df_missing.isna()[k_j].to_numpy()  \n",
    "        \n",
    "    #Computation of iterative FAMD\n",
    "    ifamd_df = IterativeFAMDImputer(n_components=n_components, data=df_missing, k1=k1, k2=k_j, nb_values_per_cat = nb_val_per_car)\n",
    "    ifamd_df.impute(n_it)\n",
    "    df = ifamd_df.df\n",
    "    print(df)\n",
    "\n",
    "    # We encode categories into 0,1\n",
    "    res = ifamd_df.df[ifamd_df.k2].copy()\n",
    "    pos = 0\n",
    "    for h in range (len(idx_k2)) :\n",
    "        col = [idx_j[pos+i] for i in range (nb_values_per_cat_df[h])]\n",
    "        res[\"max_value\"] = ifamd_df.df[col].max(axis = 1)\n",
    "        for value in col:\n",
    "            res[value] = (res[value] == res[\"max_value\"]).astype(int)\n",
    "        pos += nb_values_per_cat_df[h]\n",
    "    res = res[ifamd_df.k2] \n",
    "\n",
    "    #Compute metrics \n",
    "    print(metric_fc(res[Categ_missing], df_true[k_j][Categ_missing]))\n",
    "\n",
    "    # For continuous variables: \n",
    "    print(compute_nrmse_weighted(df[k1][C0_missing].astype(int), df_true[k1][C0_missing]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "compute_nrmse_weighted(df[k1][C0_missing], df_true[k1][C0_missing])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = df_true[k1][C0_missing].sum(axis=0)\n",
    "std_df = df_true[k1][C0_missing].std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrmse_numerator = 0 \n",
    "for c in df_true[k1][C0_missing].columns.to_numpy(): \n",
    "    nrmse_numerator+= (weights[c] * ((df_true[k1][C0_missing][c] - df[k1][C0_missing][c])/std_df[c])**2).sum()\n",
    "# Compute NRMSE denominator\n",
    "nrmse_denominator = np.sum(weights)\n",
    "# Compute NRMSE\n",
    "nrmse = np.sqrt(nrmse_numerator / nrmse_denominator)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
